# Install necessary libraries (if not already installed)
!pip install -q tensorflow

# Import libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the IMDB dataset
vocab_size = 10000  # Number of words to consider from the dataset
max_length = 200    # Maximum review length (in words)

# Load dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)

# Pad sequences to have the same length
x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')
x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')

# Build Deep Neural Network
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length),  # Embedding layer
    GlobalAveragePooling1D(),                                                 # Pooling layer to reduce dimensions
    Dense(64, activation='relu'),                                              # Hidden layer
    Dense(1, activation='sigmoid')                                             # Output layer (sigmoid for binary classification)
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(x_test, y_test, verbose=1)
print(f"Test Accuracy: {accuracy:.2f}")

# Predict on test data (optional)
y_pred = model.predict(x_test)
y_pred_classes = (y_pred > 0.5).astype("int32")

# Show classification report (optional)
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_classes))
